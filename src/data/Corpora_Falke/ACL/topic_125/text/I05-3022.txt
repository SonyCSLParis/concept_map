1 Introduction tation system presented in this bakeoff began in
Feb. this year, and will last for one year with the support of the ILAB Beijing initial project within France Telecom R&D.
Although the project last only half year by now, the main components of the system has been implemented, including code identification and conversion, basic segmentation, factoid detection, morphological analysis, name entity identification, segmentation standards adaptor, and conversion and segmentation standards adaptors, other components are integrated in a statistical framework of n-gram language model.
2 System Description
For processing both Simplified and Traditional
Chinese text from a variety of locales, including
Mainland China, Hong Kong and Taiwan, we choose UTF-8 as internal character representation within the system. The ability to transparently handle Chinese text from any Chinese locale greatly simplifies the logic of the segmentation system.
morphological words, factoids, name entities.
These types of words are processed in different ways in our system, and are incorporated into a unified statistical framework of the trigram language model.
vidual characters. These characters and the character strings are then looked up in a lexicon. For the efficient search, the lexicon is represented by a TRIE compressed in a double-array data strucefficiently by browsing the TRIE whose root represents its first character.
such as time, date, money, etc. All the factoid words are represented as regular expressions, and compiled into a compressed DFA with the row-index algorithm.
As (Wu 2003) discussed in the paper, it is those morphologically derived words (MDWs hereafter) that are most controversial and most likely to be treated differently in different standards and different systems. In our system, there are six main categories of morphological processes, affixation, directional verb, resultative verb, splitting verb, reduplication and merging, and we employ a chart parsing algorithm augmented with word lattices structure which incorporates the morphological rules especially designed for
Chinese languages with restrictive CFG.
2.2.4 Name entity identification types of NEs, namely, personal names (PERs), location names (LOCs) and organization names
(ORGs). For Chinese person names, we only consider PN candidates that begin with a family name stored in the family name list and follow a given name which is of one or two characters long. For transliterations of foreign person names, a PN candidate would be generated if it contains only characters stored in a transliterated character list. For location names and organizations names, we only use the LN list and ON list to generate the candidates.
2.3 Segmentation standards adaptor tion of an ordered list of transformations on the output of our segmentation system. The method we use is Transformation-Based Learning, and the transformation templates are lexicalized templates. In our system, we designed 14 lexicalized templates.
2.4 Speed process, the speed of segmentation is very fast.
On a single 2.80 GHz, 1G bytes memory, Xeon machine, the system is able to process about
0.73 Mega bytes per second.
The speed may vary according to the sentence lengths: given texts of the same size, those containing longer sentences will take more time.
The number reported here is an average of the time taken to process the test sets of the eight tracks we participated in.
210,319 entries, 165,103 entries, 174,268 entries,
165,655 entries respectively on AS-open, HK- open, MSR-open, PK-open tracks, which include the entries of 2,430 MDWs, 12,487 PNs,
22,907 LNs and 29,032 ONs, 10,414 fourcharacter idioms, plus the word lists generated from the training data provided by the bakeoff.
We use the training data provided by the bakeoff for training our trigram word-based language model. We also used a family name list (which contains 399 entries in our system), and a 1,021- entry transliterated name character list.
3.2 Closed tracks only be generated from the training data provided by the bakeoff. We could only use the training data provided by the bakeoff for training our word-based language model. Also, since the training data we used is only from the bakeoff, there does not exist any different standards, standards adaptor component is not necessarily needed.
as the factoid detection and NE identification can be switched on or off, so that we can investigate the relative contribution of each component to the overall word segmentation performance. The results are summarized in the table 1. For comparison, we also include in the table (Row 1) the results of using FMM. Row 2 shows the baseline results of our system, where only the lexicon is used. Each cell in the table has six fields. From the top, there are respectively Precision, Recall, F-measure, OOV Recall,
IV Recall and Speed (Mega bytes/second). We don't list the speed in Row 6 since it decreases a factor of 10 to 60 because of application of thousands of TBL rules.
1. FMM
Table 1. Our system results on all the tracks.
From Table 1 we can find that, in rows 1 and 2, the dictionary-based methods already achieve quite good recall, but the precisions are not very good because they cannot correctly identify unknown words that are not in the lexicon such as factoids and name entities. We also find that even using the same lexicon, our approach that is based on the N-gram language models outperforms the greedy approach because the use of context model resolves more ambiguities in segmentation. As shown in Rows 3 to 5, when components are switched on in turn, the overall word segmentation performance increases consistently. The morphological analysis has no contribution to the overall performance in Row
4. The main reason is that the number of MDWs used in our system is very small (only 2,430) and there may exist very small MDWs in the test sets. The similar cases occur on NE identification in the close tracks in Row 5 since we would not do NE identification at all in the close tracks.
We also notice that the contribution of NE identification is very little in the open tracks, which shows that the performance of NE identification is not very good in our system, and explains why our OOV recall is not very high compared one area of our future work to improve. The results of standards adaptation on four bakeoff test sets are shown in Row 6. It turns out that performance except IV recall improves slightly across the board in all four test sets. The main reason is that the training data and lexicon we used are mainly from the four providers in the bakeoff, there does not exist any different segmentation standards.
4 Conclusions pants, the one main reason is that the wordbased language model we used is not competitive compared with other algorithms in the closed tracks. One area of our future work is to apply other machine learning algorithm, like
Maximum Entropy (ME), Support Vector Machine (SVM), Conditional Random Field (CRF), etc.
