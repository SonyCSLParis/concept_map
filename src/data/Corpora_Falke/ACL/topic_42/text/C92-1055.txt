1. Introduction
Ambiguity resolution has long been the focus in natural language processing. Many rule-based approaches have been proposed in the past, However, when applying such approaches to large scale applications, they usually fail to offer satisfactory pertbrmance. As a huge amount of fine-grained knowledge is required to solve the ambiguity problem, it is quite difficult for rule-based approach to acquire the huge and fine-grained knowledge, and maintain consistency among them by human [Su 90a].
Probabilistic approaches attack these problems by providing a more objective measure on the preference to a given interpretation. Then, these approaches acquire huge and fine grained knowledge, or parameters in statistic terms from the corpus automatically. The uncertainty problem in linguistic phenomena is resolved on a more solid basis if a probabilistic approach is adopted. Moreover, the knowledge acquired by the statistical method is always consistent because the knowledge is acquired by jointly considering all the data in the corpus at the same time. Hence, the time for knowledge acquisition and the cost to maintain consistency are significantly reduced by adopting those probabilistic approaches.
To resolve the problems resulting from syntactic ambiguities, a unified statistical approach for ambiguity resolution has been proposed by Su [Su 88, 92b]. In that approach, all knowledge sources, including lexical, syntactic and semantic knowledge, are encoded by a unifiedprobabilistic score function with a uniform formulation. This uniform probabilistic score function has been successfully applied in spoken language processing [Su 90b, 91b, 92a] and machine translation systems [Chen 91] to integrate different knowledge sources for ambiguity resolution.
In implementing this unified probabilistic score function, values of score functions are estimated from the data in the training corpus. However, due to the problem of insufficiency of training data and incompleteness of model knowledge, the statistical variations between the training corpus and the real application are usually not covered by this approach. Therefore, the performance in the testing set sometimes gets poor in the real application.
To enhance the capability of discrimination and robustness of those proposed score function, a discrimination-oriented adaptive learning is proposed in this paper. And then, the robustness of this proposed adaptive learning procedure is enhanced by enlarging the margin between the correct candidate and its confusing candidates to achieve maximum separation between different candidates.
Since the implementation of this adaptive learning procedure is based on the uniform probabilistic score function, we will first briefly review the unified probabilistic score function. Readers who are
ACTES DE COLING-92, NANTas, 23-28 AO~r 1992 3 5 2 PROC. OF COLING-92, NANTES, AuG, 23-28. 1992 interested in the details about the uniform probabilistic score function please refer [Chen 91, Su 91b, 92a, 92b].
2. Overview of Uniform Probabilistic Score Function
2.1, General Definition
A Score Function for a given syntactic tree, say Synj, is defined as follows:
S~or~ (s~,,~)_-- v (%, , . L~, IG ' ) (]) where w~ ~ is the input word sequence, w~ ~ = {Wl,W2, '" ,w, ,} , and Lcx j , the corresponding lexical string, i.e., part of speech sequence {cjl , c j2, . . . , cj,~ }. By applying the multiplication theorem of probability, l '(Sun), Lexj I w?) can be restated as follows.
P (Syn:, l,e~j I G')
The two components, s,~,, (.','yn D and s~,~ ( Lexj ), m thc above formula are called syntactic Score Function and Lexieal Score Function, respectively. The original score function, i.e., P(Sy%,Lexjlw~'), is then called Integrated Score Function.
Next, we assume the information, from the word .sequence w] ~, required for syntactic ambiguity resolution, has percolated to the lexical interpretation Lez j . Also, only little additional information can be provided from w] ~ for the task of disambiguating syntactic interpretation ~yuj after the lexical interpretation Lex j is given. Thus, rite syntactic score can be approximated as shown in t~t.(3):
.'~.,~,, (Syn:)= l '(Sy% I Lex~,w'~ ') ~ I'(Sy% I Le%). (3)
The integrated score function P(Sy%, Lez~]wi') is then approximated as follows.
P (Synj, Lex~ I w~') (4) -~ P(Sy% I Lez,) × P(Lex, I u,;').
Such a formulation "allows us to use both lexical and syntactic knowledge in assigning preference measure m a syntactic tree. in the real computation, log operation is used to convert the operations of multiplication to the operations of addition. The following equation shows the final form in the real application.
log P ( Syrtj, Lex~ I w[') = log S,u, ( Sy% )+log S~,, ( Lex~ ) . (5)
2.2. Lexical Score F'unction
Let ck~, denote the k-th sequence of tile lexical category, or part of speech, co~xesl)onding to the word sequence w~L The Lexical Score Function can be expressed as follows [Chen 91, Su 92b]:
= f i e (~, I~ki ...... ;'), (6) i= l where ck~ is rite lexic',d category of w i. Several forms lGars 87, Chur 88, Su 92b] tor 1' (~k, I~q . . . . . p) were propo~d to simplify the computanon. For example, IChur 88] approximated "(~k IG . . . . . . ; ' )by [P(~k, I%,-,] × t'(ck, 1~,)] , A general nonlinear smoothing l'onn [Chen 91] described in Eq.(7) is adopted in this paper: u (P (~, I ~ . . . . . . ,)) (7) ~ Ag(P(, :k. ]w, ) )+( l .~)9 (Ck, [ . . . . . ), where A is the lexical weight (A = 0.6 is used in the current setup), and 9 is a transform function (log (.) is used in this paper). Hence, given both Eq.(6) and (7), the lollowing tormula is derived: log (St~ ( Lr, z~,)) n
= ~ {Atog P(~,l,,,) + (~ -~) o~°(~ I . . . . . )} J= l
(8) It is noted that the above generalized form reduced to the formulation of [Chur 88] when file transtonn function is log function and A is 0.5.
2.3. Syntactic Score Function
To show the computing mechanism for the syntactic score, we take the syntax tree in Fig.1 as an example. The syntax tree is decomposed into a number of phrase levels. Each phrase level (also called a sententialform) consists of a set of symbols (terminal or nontenninal) which can derive all the temfin',d symbols in a sentence. Let label t i in Fig.l be the time index lot each state transition of a LR parser, and Li be the i-fit phrase level. Thus, a transition from phrase level L i to phrase level Li+ 1 is equivalent to a redue action at time ti.
A ACIION
~ I . - {II I ' . O] ...... ....................
b I~ F G IA . [B C3. C41 ...... 11 ............ CA ....
I I I I I1 17" 14 ~ 1['21 II~ C2' C3' C4I ..... 1[~ . . . . . . . . . . . . . . . . . . C I C 2 C~ 124 L I~ 1121. Ca, C~, C4I ..... D ' ........... C2"" ... . . . . . . . . . . . . . . . . . C1 ....
Figure I The dr, composition of a syntax tree hito phrase levels.
ACfES DE COLING-92, NANTF.S, 23-28 AOtTr 1992 3 5 3 Pgoc. OF COLING-92, NANri~s, AUG. 23-28, 1992
The syntactic score of the syntax tree in Fig.l is then defined as
S,y, (svna) =- P(Ls,Lr, ... ,L, I L~) 8 8
: ( ) I I e L, I L~-' i=~ i=2
(9) where syn A is the parse tree, and LI through L 8 represent different phrase levels. Note that the product terms in the last formula correspond to the rightmost derivation sequence in a general LR parser [Su 91c], with left and right contexts taken into account. Therefore, such a formulation is especially useful for a generalized LR parsing algorithm, in which context-sensitive processing power is desirable.
Although the context-sensitive model in the above equation provides the ability to deal with intra-level context-sensitivity, it falls to catch interlevel correlation. In addition, the formulation of Eq.(9) gives rise to the normalization problem for ambiguous syntax trees with different number of nodes. An alternative to relieve this problem is to compact multiple highly correlated phrase levels into one in evaluating the syntactic scores. Tile formulation is expressed as follows [Su 91c]:
S,v, (sv,,a) vi (intransitive verb) saw vt (txansitive verb) art (article) a
Table 1 Categories for words and their log word-to-category scores.
In Table 1, the log word-to-category score, log (P (e ] to)), for each word is estimated from the training corpus by calculating their relative frequencies. For exanrple, in the training corpus, the word 'T ' is used as pronoun for 60 times, and 40 times as noun. Then, the log word-to-category scores can be calculated as follows.
60 lOglo l" (prou l {I} ) = loglo ( 6-'~--~-~ ) = --0.22, (,o) IOg,o P(u I (1}) = loan ~ = -0 .39 . (11)
In this example, there are 2"2"2"1=8 possible different ways to assign lexical categories to the input
P(L~,LT,L~ [ L~) × P(Lr~ I L4) x P(L4,L~ I L~) × P(L7 I LQsentence. When these 8 possible lexical ,sequences P(La I Lr,) × P(Ln I L4) × P(L4 I L~) × I' (L~, I Lt)
(10) Because thc number of shifts, i.e., the number of terms in Eq.(lO), is always the same for all ambiguous syntax trees, the normalization problem is then resolved. Moreover. it provides a way to consider both intra-level context-sensitivity and inter-level correlation of the underlying context-free grammar. With such a score function, the capability of context-sensitive parsing (in probability sense) can be achieved with a context-free grammar.
3. Discrimination and Robustness Oriented Adaptive Learning
3.1. Concepts of Adaptive Learning
The general idea of adaptive learning is to adjust the model parameters (in this paper, they are lexical scores and syntactic scores) to achieve the desired criterion (in our case, it is to minimize the error rate). To explain clearly how the adaptive learning works, we take the sentence "1 saw a man." as an example. The lexical category (i.e,, part of speech) and its corresponding log score for each word are listed in Table 1.
are parsed, only four of them are accepted by our parser. They are listed as follows:
1. pron vt art n 2. n vt art n 3. pron vi prep n 4. n vi prep n.
The syntactic scores of different parse trees are then calculated according to Eq.(10). A parse tree corresponding to the lexical sequence "[pron vt art n]" is drawn below as an example.
s
Ult n
The log syntactic scores for those four grammatical inputs are computed and listed in Table 2.
AcrEs DE COLING-92, NANTES, 23-28 Aot3"r 1992 3 5 4 PROC. OF COLING-92, NANTES, AUG. 23-28, 1992
Input Lexlcal Sequence log syntactic sco~e
[pron vt art n] (-0.7)+(41.3)+(-0.3)+(-0.2) = -I.5
[, vt art n] (41.2)+(-0.3)+(-0.3)+(-0.2) = -1.0
[prot, vi prep n] (-0.7)+(-0.7)+(-0.4)+(-0.3) = -2.1
[n vi prep ,1 (-0.2) +(-0.7)+(-0.4)+(-0.3) = -1.6 1 "fable 2 log syntactic scores of the grammatical
Input lexlcal sequences.
Accord ing to Eq.(5), the total log integrated score ( log Slez+logSsvn) for each parsed sentence hypothesis is calculated. For example, the log lexical score for "I /[pron] saw/[vt] a/[art] man/ In]" = (- 0.22-0.16-0.02-0)= -0.4. Finally, the log integrated scores for the above grammat ica l inputs are listed as follows: candidate-1, log integrated score = (-0.40-1.5 = -1.90) : l/Ipron] saw/[vt] a/[art] man/In] candidate -2. logintegrated score = ( 0.57-1.0 = -1.57) : l/in] saw/[vt] a/[art] maa/]nl candidate-3, log integrated score = (-2.04-2.1 = 4.71) : I/[pron] saw/lvi] a/[prep] man/[n] candidate -4. log integrated score = (-2.21-1.6 = -3.81) : l/[n] saw/[vii a/[prepl man/[nl
Among these four candidates, the candidate 1 is regarded as the desired selection by linguists. Since our decision criterion will select the candidate which has the highest integrated score, i.e., the second one; I/[n] ~w/ [v t ] a/[art] man/[n], it results in a decision error in this case.
To remedy this error, adaptive learning procedure is adopted to adjust the score values iteratively, including lexical and syntactic scores, until the integrated score o f the correct candidate (i.e., candidate 1) raises to the highest rank. In this paper, parameters which are adjusted by adaptive learning procedure are those log scores, including log P (c/q I wi), l ogP(ek , ]ck , - i ~ and logP(L i l L~- l ) . The amount of adjutstment in each iteration depends on the misclassihcation distance. Misclassification distance is defined as the difference between the score o f the top candidate and that o f the correct one. (In the above example, distance = (score o f correct candidate)-(score o f top candidate) = (-1.90)- (-1.57)= -0.33). From iteration to iteration, the parameters (both lexical and syntactic scores) are adjusted so that the integrated score of the correct candidate is increased, and the integrated score of the wrong candidate is decreased at the same time. The learning procedure for a sentence is stopped when the candidate o f this sentence is correctly selected. To make the explanation of this adaptive learning procedure clear, we assume lexical scores are unchanged during learning. That is, only the paranteters of the syntactic scores are adjusted. The details o f adaptive leanl ing for adjusting syntactic scores are listed as follows: hdtlal 'lzution candidate -1. zx syntactic score = [-0.7 -0.3 -0.3 -0.2] = -1.5. log integrated score = -1.9; candidate .2. " syntactic score = [-0.2 -0.3 -0.3 -0.2] = -1.0, log integrated score = -1.57; candidate -3. syntactic score = [-0.7 -0.7 -0.4 -0.3] = -2.1. log integrated score = .4.71; catudidate -4. syntactic score = [-0.2 -0.7 -0.4 -0.3] = -1.6. log integrated score = -3.81;
Iteration I candidate -1. ~ syntactic score = [-0.5 .0.3 -0.3 -0.2] = -1.3, log integrated score = -1.7; candidate -2. " syntactic score = [-0.3 -0.3 -0.3 .0.2] = -1.1, log integrated score = -1.67; candidate -3. syntactic score = [-0.5 -0.7 -0.4 ~0.3] = -1.9, log integrated score = -3.94; candidate -4. syntactic score = [-0.3 -0.7 -0.4 -0.3] = -1.7, 1o 8 integrated score = -3.91;
Iteration 2 candidate -1. e," syntactic score = [-0.2 -0.3 -0.3 -0.2] = -1.0, log integrated score = -1.4; (stop learning) candidate -2. syntactic score = [-0.6 -0.3 -0.3 -0.2] = -1.4, log integrated score = -1.97; candidate -3. syntactic score = [-0.2 -0.7 -0.4 -0.31 = -1.6, log integrated score = -3.64; candidate 5. syntactic score = [41.6 -0.7 -0.4 -0.3] = 2.0, log integrated score = -4.21;
(where * denotes the top candidate, and A denotes the desired candidate)
It is clear that after the second iteration, paranaeters have been adjusted so that the desired candidate (i.e., candidate 1) would be selected.
3.2. P rocedure o f D isc r iminat ion Learn ing
Since correct decision only depends upon correct rank ordering of the integrated scores for all ambiguit ies, not their real value, a discriminationoriented approach should directly pursue correct rank ordering. To der ive the discr imination function, the probabil ity scores ment ioned above are first jointly considered. Then, a discrimination-oriented function, namely g (.), is defined as a measurement of above mentioned score functions, so that it can well p re~rve the correct rank ordering [Su 91a]. Here, 9 ( ' ) is chosen as the weighted sum of log
ACRES DE COLING-92, NANTES, 23-28 aofrr 1992 3 5 5 PROC. OV COLING-92, NANTES. AUO. 23-28, 1992 lexical and log syntactic scores, i.e., a (sv,,k)
= wl~, . log S l ,x (Le~:k) + W,vn ' log S .~, . (Sun~:) u
. . . . . ..... :.)+ . . . . . i= l n n
(12) where .h,, (i) = logP (ckA%; . . . . 1'), and A,v,, (i) = logP(LdLi- :) . Both stand for the log lexical score and the log syntactic score of the i-th word for the k-th syntactic ambiguity, respectively. In addition, Wle z and Wsyn correspond to the weights of lexical and syntactic scores, respectively.
If the parse tree of a sentence is misselected, the parameters (i.e., the lexical and the syntactic scores) are adjusted via the proposed adaptive learning procedure. Otherwise, no parameters would be adjusted. When misselection occurs, the misclassification distance, dso is less than zero. This misclassification distance is defined as the difference between the log integrated .score of the correct candidate and that of the top one. A specific term of the syntactic score components in the (t+l)-th itera-
A(t+l) lion of the correct candidate, say sv,* (j), would be adjusted as follows:
{ ~('+'~ ' ~ - L~). (i) + ~(.;),, (j) a,. < o, ,vt3)- ' - (13)
At the same time, the term of the syntactic score components of the top candidate would be adjusted according to the following formulas: fa(.~+,,')(i) x(') "" ~a(') = ,~s) - ,~,,(j), a,~<o, (I,1) where AA~ n ( j ) is the amount of adjustment. This value is represented as zx~i~). O) = "d . . . . . . . I=] " leX t l J + iOst/n "
(15) where do is a constant which stands for a window size, and e is the learning constant for controlling the speed of convergence. The learning rule for adjusting the lexical scores can be represented in a similar manner. Notice that only the parameters of the top candidate and those of the correct candidate would be adjusted when misselections occur. Those parameters of other wrong candidates would not be adjusted in this adaptive leaming procedure. From Eq.(13), (14) and (15), it is clear that the score of the correct candidate will increase and that of wrong candidate will decrease from iteration to iteration until the correct candidate is selected. For the purpose of clarity, the detailed derivations of the above adaptive learning procedure will not be given hem. Interested readers can contact the authors for details.
3.3. Robustness Issues
Since it is easy to improve the performance in a training set by adopting a model with a large number of parameters, the error rate measured in the training set frequently turns out to be over-optimistic. Moreover, the parameters estinlated from the training corpus may be quite differ from that obtained from the real applications. These phenomena may occur due to the factors of finite sampling size, style mismatch, or domain mismatch, etc. To achieve a better perlbrmance in the real application, one must deal with the possible mismatch of parameters, or statistical variation between the training corpus and the real application. One way to achieve this goal is to enlarge the inter-class distance to achieve maximum separation [Su 91a] between the cot:ect candidate and the other candidates. That is, this approach provides a tolerance zone between different candidates for allowing possible data scattering in the real application.
Traditional adaptive learning methods [Amar 67, Kata 90] stop adjusting parameters once the input pattern has been correctly classified. However, if we stop adjusting parameters under the condition that the observations are correctly classified in the training corpus, the distance between the correct candidate and other ambiguities may still be too small. Thus, it is vulnerable to deal with possible modeling errors and statistical variations between the training corpus and the real application. Su [Su 91a] has proposed a robust learning procedure which continues to enlarge the margin between the correct candidate and the top one, even if the syntax tree of the sentence has been correctly seleeted. That is, the parameters will not be adjusted only if the distance between the correct candidate and the others has exceeded a given threshold. The learning rules in Eq.(13), (14) are then modified as follows. If dsc _< 6, where 6 is a preset margin, the syntactic score in the (t+l) iteration tor the correct candidate is adjusted according to the following formulas:
( ,~( t+ l ) l l ) (t) ~,~, I : ) = .t,~., OI + A; , .~. O), a,, < ~, (16) ...... O) ,x~,'~),, 0) , ot/,~T,o,~.
And, the syntactic score of the top candidate is adjusted as follows:
~L~,t, '~Ol -~"~ O l - ,ax~, ;LO) a,~<~, - -,v . . . . (17) ) .{t+l),., - - 1(t) La'~" t31- ",~, O), otherwtse.
ACa-ES DE COLING-92, NANTES, 23-28 xot:rr 1992 3 5 6 I'ROC. OF COLING-92, NANTES, AUG. 23-28, 1992
4. Simulations
The following experiments are conducted to ilivestigate the advmltage of the proposed discrimination and robustness oriented adaptive leanfing pro_ cedure. In the experiments, 4,000 sentences, which are extracted from IBM tectmical manuals are first associated with their conesponding correct category sequences and correct parsed trees by linguists. The corpus are then partitioned into a training corpus of 3,2(X) sentences and a test set of 800 sentences. Next, the lexical and syntactic probabilities are estimated from the data in the training corpus. Afterwards, the sentences in the test set a~e used to evaluate rite perlomtance of the proposed algorithm using the estimated lexical and syntactic probabilities. This integrated score timction apploach using the estimated probabilities is considelcd as the baseline system. Performances of discrimination ufi~ ented adaptive learnings with and widtout robustness enhancement are then evaluated. The accuracy rate of the syntactic ambiguity resolution for the training corpus and rile test set are summarized in Table 3. (Note that the top candidate is selected t?om all po~ible parses allowed by tile gmntmars of the system; therefore, the baseline perlonnance is evaluated under a highly ambiguous environment.)
I laseline
Training I Test Set CorDus I
79.75 I 46.(X)
95.50 I 56.88
96.03 I 60.62
%) of syntactic disanl|flguatlun
Table 3 shows that syntax tree accuracy ~ate is improved from 46% to 56.88% using the basic version of discrimination oriented adaptive learning procedure. This significant iiuprovement shows the superiority of the adaptive learning procedure for dealing with the disambiguation task. l:unhenuore, when the rt)bust versiou of leanfiug proccdure is adopted, the perfommnce is iml)roved further (flora 56.88% to 60.62%). It means that the rohuslness of the learning procedure is indeed enhanced by enlarging the distance between the correct candidate and other candidates. Moreover, not only the accuracy rate of syntax trec is improved using adaptive learning, but al~ that of lexical sequence is improved. In this paper, a lexical sequence is ~egarded as "correct" only if all the lexical categories in a sentence perfectly match those selected by linguists. In other words, we are measuring "sentence accurac3: rate" in contract tu "word accuracy rate" as adopled ill [Chur 88, Gal~ 87]. Table 4 shows that file basic version of adaptive leaming procedule ilnproves the sentence accuracy rate of lexicol sequences a|xmt 5% (from 77.12% to 82.38%). Again, with the robust version of learning, the accurate rate of lexical sequences is greatly enbanced.
I Trahdng Corpu~
+ It asit i version of learn big 98.91 8~_2.3~]
+ ltohust version of 98.53 1 Learning L _ _
Table 4 ~'-;entencc accuracy rate (in %) of lexlcal sequences
The behavior of cach iteration of the adaptive learning process is shown in Figure 2. Through observing this figure, we can conclude that if the robustness issues are not considered during learning, the performance of tile test set would decrease as the training t)~ocess goes on. This is the phenomena of river-tuning, llowever, by h~rcing the learning procedure to continue unlil the separation between the correct candidate and the top one exceeds the desired margin, the performmlce of the test set can be t'utther improved, and no degradation phenomenon is observed.
I ............. = ; : : . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
l:lgurc 2 Syntax t~e accuracy rate versus Iteratkms for basic and r~lbusl verslou of adaptive learning
AcrEs DE COLING-92, NAbrrl,:s, 23 28 Ao(rr 1992 3 5 7 l'lto¢:, oi: C()l,IN(;-92, N^yn~s, AUG. 23-28, 1992
5. Summary
Because of insufficient training data, and ap- [Su 88] proximation error introduced by the language model, traditional statistical approaches, which resolve ambiguities by indirectly and implicitly using maximum likelihood method, fail to achieve high performance in real applications. To overcome these [Su 90a] problems, adaptive learning is proposed to pursue the goal of minimizing discrimination error directly. The performance of syntactic ambiguity resolution is significantly improved using the discrimination [Su 90b] oriented analysis. In addition, the sentence accuracy rate of the lexical sequences is also improved. Moreover, the performance is further enhanced by using the robust version of learning procedure, which enlargeds the margin between the correct candidate and its candidates. The final resuits show that using the basic version of learning, [Su 91b] the syntax tree selection accuracy rate is improved about 10% (from 46% to 56.88%), and the total improvement is over 14% using robust version learning. Also, the sentence accuracy rate for lexical sequences is improved from 77.12% to 82.38 and 87.88% using the basic and robust version of leaming procedure, respectively.
Reference
[Amar 67]
[Chen 91]
[Chur 88]
[Gars 87]
[Kata 90]
[Su 91at
Amari S., "A theory of adaptive pattern classifiers," IEEE Trans. on Electronic Computers, Vol. EC-16, pp. 299-307, [Su 91c] June 1967. Chen S.-C., J.-S. Chang, J.-N. Wang, and K.-Y. Su, "ArchTran: A Corpusbased Statistics-oriented English-Chinese Machine Translation System," Proc. of [Su 92a] Machine Translation Summit II1. Washington, D. C., U.S.A., July 1-3, 1991. Church, K., "A Stochastic Parts Program and Noun Phrase for Unrestricted Text," ACL Proceedings of 2nd Conference on Applied Natural Language Processing, pp.136-143, Austin, Texas, U.S.A., 9-12 Feb. 1988. Garside, R., G., Leech, and G., Samp- [Su 92b] son, "The Computational Analysis of English: A Corpus-Based Approach," London: Longman. S.Katagiri. C.H. Lee, "A Generalized Probabilistic Decent Method," Proc.
Acous. Sco. of Japan, 2-p-6, pp. 141-142, Nagoya, Sept 1990. Su K.-Y., and J.-S. Chang, "Semantic and Syntactic Aspects of Score Function," Proc. COLING-88, Vol.2, pp. 642-644, 12th Int. Conf. on Comput. Linguistic, Budapest, Hurgay, 22-27, Aug. 1988. Su, K.-Y., and J.-S Chang, 1990. "Some Key Issues in Designing MT Systems," Machi'ne Translation, vol. 5, no. 4, pp. 265-300, 1990. Su K.-Y., T.-H. Chlang and Y.-C. Lin, "A Unified Probabilistic Score Function for Integrating Speech and Language Information in Spoken Language Processing," Proceeding of 1990 International Conference on Spoken Language Processing, pp.901-904, Kobe, Japan, 19-22 Nov. 1990. Keh-Yih Su, Tung-Hui Chiang and Yi- Chung Lin, "A Robustness and Discrimination Orientexl Score Function for Integrating Speech and Language Processing," Proceeding of the 2rid European Conference on Speech Communication and Technology, Genova, Italy, pp. 207-210, Sep. 24-26 1991. Su K.-Y., and C.-H. Lee, "Robustness and Discrimination Oriented Speech Recognition Using Weighted HMM and Subspace Projection Approaches," Proc. ICASSP- 91, pp.541-544, Toranto, Canada, 14-17 May, 1991. Su K.-Y., J.-N. Wang, M.-H. Su, and J.- S. Chang, "GLR Parsing with Scoring," in Tomita, Masaru (ed.), Generalized LR Parsing, Chapter 7, pp.93-112, Kluwer Academic Publisher 1991.. Keh-Yih Su, Tung-Hui Chiang and Yi- Chung Lin, "An Unified Framework to Incorporate Speech and Language Information in Spoken Language Processing" to appear in the Proceeding of IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP-92, San Francisco, California, U.S.A., March 23- 26 1992. Sn K.-Y., J.-S. Chang and Y.-C. Lin, "A Unified Approach to Disambiguation Using A Uniform Formulation of Prohabilistic Score Function," in preparation.
