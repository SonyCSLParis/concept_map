1 Introduction
Automatic evaluation metrics for Machine Translation (MT) systems, such as BLEU (PAPINENI et al., 2001) or NIST (DODDINGTON, 2002), are now well established. They serve as quality assessment methods or comparison tools and are a fast way of measuring improvement. Although it is claimed that such objective MT evaluation methods are language-independent, they are usually only applied to English, as they basically rely on word counts. In fact, the organisers of campaigns like NIST (PRZYBOCKI, 2004)1, TIDES2 or IWSLT (AKIBA et al., 2004)3, prefer to evaluate outputs of machine translation systems which are already segmented into words before applying such objective evaluation methods. The consequence of this state of affairs is that evaluation campaigns of English to Japanese or English to Chinese machine translation systems for instance, are not, to our knowledge, widely seen or reported.
2 Overview
2.1 The word segmentation problem As statistical machine translation systems basically rely on the notion of words through their lexicon models (BROWN et al., 1993), they are usually capable of outputting sentences already segmented into words when they translate into languages like Chinese or Japanese. But this is not necessarily the case with commercial systems. For instance, Systran4 does not output segmented texts when it translates into Chinese or Japanese.
As such, comparing systems that translate into languages where words are not an immediate given in unprocessed texts, is still hindered by the human evaluation bottleneck. To compare the performance of different systems, segmentation has to be performed beforehand.
1http://www.nist.gov/speech/tests/mt- /doc/mt04 evalplan.v2.1.pdf
One can always apply standard word segmentation tools (for instance, The Peking University Segmenter for Chinese (DUAN et al., 2003) or ChaSen for Japanese (MATSUMOTO et al., 1999)), and then apply objective MT evaluation methods. However, the scores obtained would be biased by the error rates of the segmentation tools on MT outputs5. Indeed, MT outputs still differ from standard texts, and their segmentation may lead to a different performance. Consequently, it is difficult to directly and fairly compare scores obtained for a system outputting non-segmented sentences with scores obtained for a system delivering sentences already segmented into words.
2.2 BLEU in characters
Notwithstanding the previous issue, it is undeniable that methods like BLEU or NIST have been adopted by the MT community as they measure complementary characteristics of translations: namely fluency and adequacy (AKIBA et al., 2004, p. 7). Although far from being perfect, they definitely are automatic, fast, and cheap. For all these reasons, one cannot easily ask the MT community to give up their practical knowhow related to such measures. It is preferable to state an equivalence with well established measures than to merely look for some correlation with human scores, which would indeed amount to propose yet another new evaluation method.
Characters are always an immediate given in any electronic text of any language, which is not necessarily the case for words. Based on this observation, this study shows the effect of shifting from the level of words to the level of characters, i.e., of performing all computations in characters instead of words. According to what was said above, the purpose is not to look for any correlation with human scores, but to establish an equivalence between BLEU scores obtained in two ways: on characters and on words.
Intuitively a high correlation should exist. The contrary would be surprising. However, the equivalence has yet to be determined, along with the corresponding numbers of characters and words for which the best correlation is obtained.
5Such error rates are around 5% to 10% for standard texts. An evaluation of the segmentation tool is in fact required. on MT outputs alone.
3 Experimental setup
The most popular off-the-shelf objective methods currently seem to be BLEU and NIST. As NIST was a modification of the original definition of BLEU, the work reported here concentrates on BLEU. Also, according to (BRILL and SORICUT, 2004), BLEU is a good representative of a class of automatic evaluation methods with the focus on precision6.
3.1 Computation of a BLEU score For a given maximal order N , a baseline BLEUwN score is the product of two factors: a brevity penalty and the geometric average of modified n-gram precisions computed for all ngrams up to N .
BLEUwN score = BP × N √√√√ N∏ n=1 pn
The brevity penalty is the exponential of the relative variation in length against the closest reference:
BP =
{ 1 if |C| > |Rclosest| e1−r/c if |C| ≤ |Rclosest| where C is the candidate andRclosest is the closest reference to the candidate according to its length. |S| is the length of a sentence S in words. Using a consistent notation, we note as |S|W the number of occurrences of the (sub)string W in the sentence S, so that |S|w1...wn is the number of occurrences of the word n-gram w1 . . . wn in the sentence S.
With the previous notations, a modified n-gram precision for the order n is the ratio of two sums7: pn = w1...wn∈C |C|w1...wn
• the numerator gives the number of n-grams of the candidate appearing in the references,
6ROUGE (LIN and HOVY, 2003) would be a representative of measures with the focus on recall.
7We limit ourselves to the cases where one candidate or one reference is one sentence.
limited to the maximal number of occurrences of the n-gram considered in a single reference8;
• the denominator gives the total number of ngrams in the candidate.
We leave the basic definition of BLEU untouched. The previous formulae can be applied to character n-grams instead of word n-grams. In the sequel of this paper, for a given order N , the measure obtained using words will be called BLEUwN , whereas the measure in characters for a given order M will be noted BLEUcM .
3.2 The test data We perform our study on English because a language for which the segmentation is obvious and undisputable is required. On Japanese or Chinese, this would not be the case, as different segmenters differ in their results on the same texts9.
The experiments presented in this paper rely on a data set consisting of 510 Japanese sentences translated into English by 4 different machine translation systems, adding up to 2, 040 candidate translations. For each sentence, a set of 13 references had been produced by hand in advance.
Different BLEU scores in words and characters were computed for each of the 2, 040 English candidate sentences, with their corresponding 13 reference sentences.
4 Results: equivalence BLEUwN / BLEUcM
To investigate the equivalence of BLEUwN and BLEUcM , we use three methods: we look for the best correlation, the best agreement in judgements between the two measures, and the best behaviour, according to an intrinsic property of BLEU.
4.1 Best correlation For some given order N , our goal is to determine the value of M for which the BLEUcM scores (in
8This operation is referred to as clipping in the original paper (PAPINENI et al., 2001).
9Although we already applied the method in characters on unsegmented Japanese or Chinese MT outputs, this is not the object of the present study, which, again, is to show the equivalence between BLEU in words and characters.
characters) are best correlated with the scores obtained with BLEUwN . To this end, we compute for all possible Ns and Ms all Pearson’s correlations between scores obtained with BLEUwN and BLEUcM . We then select for each N , that M which gives a maximum in correlation. The results10 are shown in Table 1. For N = 4 words, the best M is 17 characters.
4.2 Best agreement in judgement
Similar to the previous method, we compute for all possible Ms and Ns all Kappa coefficients between BLEUwN and BLEUcM and then select, for each given N , that M which gives a maximum. The justification for such a procedure is as follows.
All BLEU scores fall between 0 and 1, therefore it is always possible to recast them on a scale of grades. We arbitrarily chose 10 grades, ranging from 0 to 9, to cover the interval [ 0 , 1 ] with ten smaller intervals of equal size. A grade of 0 corresponds to the interval [ 0 , 0.1 [, and so on, up to grade 9 which corresponds to [ 0.9 , 1 ]. A sentence with a BLEU score of, say 0.435, will be assigned a grade of 4.
By recasting BLEU scores as described above, they become judgements into discrete grades, so that computing two different BLEU scores first in words and then in characters for the same sentence, is tantamount to asking two different judges to judge the same sentence. A wellestablished technique to assess the agreement between two judges being the computation of the Kappa coefficient, we use this technique to measure the agreement between any BLEUwN and any BLEUcM .
The maximum in the Kappa coefficients is reached for the values11 given in Table 1. For N = 4 words, the best M is 18 characters.
10The average ratio M/N obtained is 4.14, which is not that distant from the average word length in our data set: 3.84 for the candidate sentences.
Also, for N = 4, we computed all values of Ms for each sentence length. See Table 2.
11Except for N = 3, where the value obtained (14) is quite different from that obtained with Pearson’s correlation (10), the values obtained with Kappa coefficients atmost differ by 1.
4.3 Best analogical behaviour BLEU depends heavily on the geometric average of modified n-gram precision scores. Therefore, because one cannot hope to find a given n-gram in a sentence if neither of the two included (n− 1)- grams is found in the same sentence, the following property holds for BLEU:
For any given N , for any given candidate, for any given set of references,
BLEUwN ≤ BLEUw(N−1) The left graph of Figure 2 shows the correspondence of BLEUw4 and BLEUw3 scores for the data set. Indeed all points are found on the diagonal or below.
Using the property above, we are interested in finding experimentally the value M such that BLEUcM ≤ BLEUw(N−1) is true for almost all values. Such a value M can then be considered to be the equivalent in characters for the value N in words.
Here we look incrementally for theM allowing BLEUcM to best mimic BLEUwN , that is leaving at least 90% of the points on or under the diagonal. For N = 4, as the graph in the middle of Figure 2 illustrates, such a situation is first encountered for M = 18. The graph on the right side shows the corresponding layout of the scores for the data set. This indeed tends to confirm that the M for which BLEUcM displays a similar behaviour to BLEUw4 is around 18.
5 The standard case of system evaluation
5.1 BLEUw4 ' BLEUc18 According to the previous results, it is possible to find some M for some given N for which there is a high correlation, a good agreement in judgement and an analogy of behaviour between measures in characters and in words. For the most widely used value of N , 4, the corresponding values in characters were 17 according to correlation, 18 according to agreement in judgement, and 18 according to analogical behaviour. We thus decide to take 18 as the number of characters corresponding to 4 words (see Figure 1 for plots of scores in words against scores in characters).
5.2 Ranking systems We recomputed the overall BLEU scores of the four MT systems whose data we used, with the usual BLEUw4 and its corresponding method in characters, BLEUc18. Table 3 shows the average values obtained on the four systems.
When going from words to characters, the values decrease by an average of 0.047. This is explained as follows: a sentence of less than N units, has necessarily a BLEU score of 0 for N - grams in this unit. Table 4 shows that, in our data, there are more sentences of less than 18 characters (350) than sentences of less than 4 words (302). Thus, there are more 0 scores with characters, and this explains the decrease in system scores when going from words to characters.
On the whole, Table 3 shows that happily enough, shifting from words to characters in the application of the standard BLEU measure leaves the ranking unchanged12.
6 Conclusion We studied the equivalence of applying the BLEU formula in character M -grams instead of word N -grams. Our study showed a high correlation, a good agreement in judgement, and an analogy of behaviour for definite corresponding values of M and N . For the most widely used value of N , 4, we determined a corresponding value in characters of 18.
Consequently, this study paves the way to the application of BLEU (in characters) in objective evaluation campaigns of automatic translation into languages without word delimiters, like Chinese or Japanese, as it avoids any problem with segmentation.
