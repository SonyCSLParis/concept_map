1 Introduction Modeling term co-occurrence is important for many natural language applications, such as topic segmentation (Ferret, 2002), query expansion (Vechtomova et al., 2003), machine translation (Tanaka, 2002), language modeling (Dagan et al., 1999; Yuret, 1998), and term weighting (Hisamitsu and Niwa, 2002). For these applications, we are interested in terms that co-occur in close proximity more often than expected by chance, for example, � “NEW”,“YORK” � , � “ACCURATE”,“EXACT” � and � “GASOLINE”,“CRUDE” � . These pairs of terms represent distinct lexical-semantic phenomena, and as consequence the terms have an affinity for each other. Examples of such affinities include synonyms (Terra and Clarke, 2003), verb similarities (Resnik and Diab, 2000) and word associations (Rapp, 2002).
Ideally, a language model would capture the patterns of co-occurrences representing the affinity between terms. Unfortunately, statistical models used to capture language characteristics often do not take contextual information into account. Many models incorporating contextual information use only a select group of content words and the end product is a model for sequences of adjacent words (Rosenfeld, 1996; Beeferman et al., 1997; Niesler and Woodland, 1997).
Practical problems exist when modeling text statistically, since we require a reasonably sized corpus in order to overcome sparseness problems, but at the same time we face the difficulty of scaling our algorithms to larger corpora (Rosenfeld, 2000). Attempts to scale language models to large corpora, in particular to the Web, have often used general-purpose search engines to generate term statistics (Berger and Miller, 1998; Zhu and Rosenfeld, 2001). However, many researchers are recognizing the limitations of relying on the statistics provided by commercial search engines (Zhu and Rosenfeld, 2001; Keller and Lapata, 2003). ACL 2004 features a workshop devoted to the problem of scaling human language technologies to terabytescale corpora.
Another approach to capturing lexical affinity is through the use of similarity measures (Lee, 2001; Terra and Clarke, 2003). Turney (2001) used statistics supplied by the Altavista search engine to compute word similarity measures, solving a set of synonym questions taken from a series of practice exams for TOEFL (Test of English as a Foreign Language). While demonstrating the value of Web data for this application, that work was limited by the types of queries that the search engine supported.
Terra and Clarke (2003) extended Turney’s work, computing different similarity measures over a local collection of Web data using a custom search system. By gaining better control over search semantics, they were able to vary the techniques used to estimate term co-occurrence frequencies and achieved improved performance on the same question set in a smaller corpus. The choice of the term co-occurrence frequency estimates had a bigger impact on the results than the actual choice of similarity measure. For example, in the case of the pointwise mutual information measure (PMI), values for ����� � � are best estimated by counting the number of times the terms � and � appear together within 10-30 words. This experience suggests that the empirical distribution of distances between adjacent terms may represent a valuable tool for assessing term affinity. In this paper, we present an novel algorithm for computing these distributions over large corpora and compare them with the expected distribution under an independence assumption.
In section 2, we present an independence model and a parametric affinity model, used to capture term co-occurrence with support for distance information. In section 3 we describe our algorithm for computing lexical affinity over large corpora. Using this algorithm, affinity may be computed between terms consisting of individual words or phrases. Experiments and examples in the paper were generated by applying this algorithm to a terabyte of Web data. We discuss practical applications of our framework in section 4, which also provides validation of the approach.
2 Models for Word Co-occurrence There are two types of models for the co-occurrence of word pairs: functional models and distance models. Distance models use only positional information to measure co-occurrence frequency (Beeferman et al., 1997; Yuret, 1998; Rosenfeld, 1996). A special case of the distance model is the n-gram model, where the only distance allowed between pairs of words in the model is one. Any pair of word represents a parameter in distance models. Therefore, these models have to deal with combinatorial explosion problems, especially when longer sequences are considered. Functional models use the underlying syntactic function of words to measure co-occurrence frequency (Weeds and Weir, 2003; Niesler and Woodland, 1997; Grefenstette, 1993). The need for parsing affects the scalability of these models.
Note that both distance and functional models rely only on pairs of terms comprised of a single word. Consider the pair of terms “NEW YORK” and “TERRORISM”, or any pair where one of the two items is itself a collocation. To best of our knowledge, no model tries to estimate composite terms of form ��������� � � or ��������� ������� where � , � , � , � are words in the vocabulary, without regard to the distribution function of � .
In this work, we use models based on distance information. The first is an independence model that is used as baseline to determine the strength of the affinity between a pair of terms. The second is intended to fit the empirical term distribution, reflecting the actual affinity between the terms.
Notation. Let ﬀ be a random variable with range comprising of all the words in the vocabulary. Also, let us assume that ﬀ has multinomial probability distribution function �ﬂﬁ . For any pair of terms � and � , let ﬃ �"! # be a random variable with the distance distribution for the co-occurrence of terms � and � . Let the probability distribution function of the random variable ﬃ��"! # be ��$%���&����� and the corresponding cumulative be '($%���&����� .
2.1 Independence Model Let � and � be two terms, with occurrence probabilities �)ﬁ����*� and �)ﬁ+���,� . The chances, under independence, of the pair � and � co-occurring within a specific distance - , �.$/���0���1 -�� is given by a geometric distribution with parameter � , ﬃ324ﬀ/5 67��-+8��1� . This is straightforward since if � and � are independent then ��ﬁ+���9 �,�;:<�=ﬁ����*� and similarly ��ﬁ>���1 �*�;: �)ﬁ+����� . If we fix a position for a � , then if independent, the next � will occur with probability �)ﬁ+�����;?@�BADCE�=ﬁ+���,�F�"G�HJI at distance - of � . The expected distance is the mean of the geometric distribution with parameter � .
The estimation of � is obtained using the Maximum Likelihood Estimator for the geometric distribution. Let K
G be the number of co-occurrences with distance - , and L be the sample size:
�M:
A
N
A
I
OQP
R
GBS)I
K
G
(1)
We make the assumption that multiple occurrences of � do not increase the chances of seeing � and vice-versa. This assumption implies a different estimation procedure, since we explicitly discard what Befeerman et al. and Niesler call selftriggers (Beeferman et al., 1997; Niesler and Woodland, 1997). We consider only those pairs in which the terms are adjacent, with no intervening occurrences of � or � , although other terms may appear between them
Figure 1 shows that the geometric distribution fits well the observed distance of independent words DEMOCRACY and WATERMELON. When a dependency exists, the geometric model does not fit the data well, as can be seen in Figure 2. Since the geometric and exponential distributions represent related idea in discrete/continuous spaces it is expected that both have similar results, especially when �MTUA .
2.2 Affinity Model The model of affinity follows a exponential-like distribution, as in the independence model. Other researchers also used exponential models for affin-
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Figure 2: ' $ � watermelon � fruits � ity (Beeferman et al., 1997; Niesler and Woodland, 1997). We use the gamma distribution, the generalized version of the exponential distribution to fit the observed data. Pairs of terms have a skewed distribution, especially when they have affinity for one another, and the gamma distribution is a good choice to model this phenomenon.
ﬀD�>VWVX�1��ﬃY:Z-+8�[\�F]ﬂ�;:
-&^�HJI_59H�GB`"a where b
��[ﬂ� is the complete gamma function. The exponential distribution is a special case with [c: A . Given a set of co-occurrence pairs, estimates for [ and ] can be calculated using the Maximum Likelihood Estimators given by:
[d]e:
(4) Figure 2 shows the fit of the gamma distribution to the word pair FRUITS and WATERMELON ( [q: r,sut t v v�w>x ).
3 Computing the Empirical Distribution The independence and affinity models depend on a good approximation to N . We try to reduce the bias of the estimator by using a large corpus. Therefore, we want to scan the whole corpus efficiently in order to make this framework usable.
3.1 Corpus The corpus used in our experiments comprises a terabyte of Web data crawled from the general web in 2001 (Clarke et al., 2002; Terra and Clarke, 2003). The crawl was conducted using a breadthfirst search from a initial seed set of URLs representing the home page of 2392 universities and other educational organizations. Pages with duplicate content were eliminated. Overall, the collection contains 53 billion words and 77 million documents.
3.2 Computing Affinity Given two terms, � and � , we wish to determine the affinity between them by efficiently examining all the locations in a large corpus where they cooccur. We treat the corpus as a sequence of terms y
= z I
�Fz"{�� sksks
�FzF| where } is the size of the corpus. This sequence is generated by concatenating together all the documents in the collection. Document boundaries are then ignored.
While we are primarily interested in withindocument term affinity, ignoring the boundaries simplifies both the algorithm and the model. Document information need not be maintained and manipulated by the algorithm, and document length normalization need not be considered. The order of the documents within the sequence is not of major importance. If the order is random, then our independence assumption holds when a document boundary is crossed and only the within-document affinity can be measured. If the order is determined by other factors, for example if Web pages from a single site are grouped together in the sequence, then affinity can be measured across these groups of pages.
We are specifically interested in identifying all the locations where � and � co-occur. Consider a particular occurrence of � at position ~ in the sequence ( zF:� ). Assume that the next occurrence of � in the sequence is zF and that the next occurrence of � is zB (ignoring for now the exceptional case where zF is close to the end of the sequence and is not followed by another � and � ). If 
 , then no � or � occurs between z� and z  , and the interval can be counted for this pair. Otherwise, if Ł let z" be the last occurrence of � before zF . No � or � occurs between z  and z  , and once again the interval containing the terms can be considered.
Our algorithm efficiently computes all locations in a large term sequence where � and � co-occur with no intervening occurrences of either � or � . Two versions of the algorithm are given, an asymmetric version that treats terms in a specific order, and a symmetric version that allows either term to appear before the other.
The algorithm depends on two access functions  and  that return positions in the term sequence z
I sksks
�Fz

©
Informally, the access function  �z¬��~@� returns the position of the first occurrence of the term z located at or after position ~ in the term sequence. If there is no occurrence of z at or after position ~ , then 
�z_��~@� returns }±¥²A . Similarly, the access function F�z_��~@� returns the position of the last occurrence of the term z located at or before position ~ in the term sequence. If there is no occurrence of z at or before position ~ , then F�z_��~@� returns r .
These access functions may be efficiently implemented using variants of the standard inverted list data structure. A very simple approach, suitable for a small corpus, stores all index information in memory. For a term z , a binary search over a sorted list of the positions where z occurs computes the result of a call to  �z¬��~@� or F�z¬��~@� in ³���hkj lmK ´µ�q³���hkj l;}e� time. Our own implementation uses a two-level index, split between memory and disk, and implements different strategies depending on the relative frequency of a term in the corpus, minimizing disk traffic and skipping portions of the index where no co-occurrence will be found. A cache and other data structures maintain information from call to call.
­
­
­
. Since each iteration generates a co-occurrence pair, the time complexity of the algorithm depends on » , the number of such pairs, rather than than number of times � and � appear individually in the corpus. Including the time required by calls to access functions, the algorithm generates all co-occurrence pairs in ³���»4hoj l}n� time.
~�·UA ; while ~M¤} do <·
Generate � ­
¥cA ; end while;
The symmetric version of the algorithm is given next. It generates all locations in the term sequence where � and � co-occur with no intervening occurrences of either � or � , regardless of order. Its operation is similar to that of the asymmetric version.
~�·UA ; while ~M¤} do  ·¸VX�>¾��

���&��~@�_�

·¸VW¿µL��F���&�F,�_��F�����F,�F� ; if �½} then
Generate � ­
¥cA ; end while;
To demonstrate the performance of the algorithm, we apply it to the 99 word pairs described in Section 4.2 on the corpus described in Section 3.1, distributed over a 17-node cluster-of-workstations. The terms in the corpus were indexed without stemming. Table 1 presents the time required to scan all co-occurrences of given pairs of terms. We report the time for all hosts to return their results.
1.5
2.5
3.5
4.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Lo glik el ih oo
-1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Lo glik el ih oo
Average 310.32 ms Slowest 744.1ms
Table 1: Scanning performance on 99 word pairs of the Minnesota Word Association Norms
4 Evaluation
We use the empirical and the parametric affinity distributions in two applications. In both, the independence model is used as a baseline.
4.1 Log-Likelihood Ratio
The co-occurrence distributions assign probabilities for each pair at every distance. We can compare point estimations from distributions and how unlikely they are by means of log-likelihood ratio test:
�6&À,ÁM:c�6&À)Â where � Ã and ��Å are the parameters for �.$/���0���,� under the empirical distribution and independence models, respectively. It is also possible to use the cumulative 'Æ$ instead of ��$ . Figure 3 show loglikelihood ratios using the asymmetric empirical distribution and Figure 4 depicts log-likelihood ratio using the symmetric distribution.
A set of fill-in-the-blanks questions taken from GRE general tests were answered using the loglikelihood ratio. For each question a sentence with one or two blanks along with a set of options Ç was given, as shown in Figure 5.
The correct alternative maximizes the likelihood of the complete sentence È :
�6&À,ÁM:c�6&À
(6) where -*�"! # is distance of � and � in the sentence. Since only the blanks change from one alternative to another, the remaining pairs are treated as constants and can be ignored for the purpose of ranking:
6&À,Á��:c�6&À
É
(7) for every �%ÎWÇ . It is not necessary to compute the likelihood for all pairs in the whole sentence, instead a cut-off for the maximum distance can be specified. If the cutoff is two, then the resulting behavior will be similar to a word bigram language model (with different estimates). An increase in the cut-off has two immediate implications. First, it will incorporate the surroundings of the word as context. Second, it causes an undirect effect of smoothing, since we use cumulative probabilities to compute the likelihood. As with any distance model, this approach has the drawback of allowing constructions that are not syntactically valid.
The tests used are from GRE practice tests extracted from the websites: gre.org (9 questions), PrincetonReview.com(11 questions), Syvum.com (15 questions) and Microedu.com (28 questions). Table 2 shows the results for a cutoff of seven words. Every questions has five options, and thus selecting the answer at random gives an expected score of 20%. Our framework answers 55% of the questions.
The science of seismology has grown just enough so that the first overly bold theories have been .
a) magnetic. . . accepted b) predictive . . . protected c) fledgling. . . refuted d) exploratory . . . recalled e) tentative. . . analyzed
Figure 5: Example of fill-in-the-blanks question
Source Correct Answers ETS.org 67%
Princeton Review 54% Syvum.com 67%
Microedu.com 46% Overall 55%
Table 2: Fill-in-the-blanks results
4.2 Skew Our second evaluation uses the parametric affinity model. We use the skew of the fitted model to evaluate the degree of affinity of two terms. We validated our hypothesis that a greater positive skew corresponds to more affinity. A list of pairs from word association norms and a list of randomly picked pairs are used. Word association is a common test in psychology (Nelson et al., 2000), and it consists of a person providing an answer to a stimulus word by giving an associated one in response. The set of words used in the test are called “norms”. Many word association norms are available in psychology literature, we chose the Minnesota word association norms for our experiments (Jenkings, 1970). It is composed of 100 stimulus words and the most frequent answer given by 1000 individuals who took the test. We also use 100 word pairs generated by randomly choosing words from a small dictionary. The skew in the gamma distribution is ÏÐ:ÒÑÌÓ>Ô [ and table 3 shows the normalized skew for the association and the random pair sets. Note that the set of 100 random pairs include some non-independent ones.
The value of the skew was then tested on a set of TOEFL synonym questions. Each question in this synonym test set is composed of one target word and a set of four alternatives. This TOEFL synonym test set has been used by several other researchers. It was first used in the context of Latent Semantic Analisys(LSA) (Landauer and Dumais, 1997), where 64.4% of the questions were answered correctly. Turney (Turney, 2001) and Terra et al. (Terra and Clarke, 2003) used different sim-
Pair Sets Ï Minnesota association norm 3.1425
Random set 2.1630
Table 3: Skewness, ÏÕ:ZÑ sÖr indicates independence ilarity measures and statistical estimates to answer the questions, achieving 73.75% and 81.25% correct answers respectively. Jarmasz (Jarmasz and Szpakowicz, 2003) used a thesaurus to compute the distance between the alternatives and the target word, answering 78.75% correctly. Turney (Turney et al., 2003) trained a system to answer the questions with an approach based on combined components, including a module for LSA, PMI, thesaurus and some heuristics based on the patterns of synonyms. This combined approach answered 97.50% of the questions correctly after being trained over 351 examples. With the exception of (Turney et al., 2003), all previous approaches were not exclusively designed for the task of answering TOEFL synonym questions.
In order to estimate [ and ] we compute the empirical distribution. This distribution provides us with the right hand side of the equation 4 and we can solve for [ numerically. The calculation of ] is then straightforward. Using only skew, we were able to answer 78.75% of the TOEFL questions correctly. Since skew represents the degree of asymmetry of the affinity model, this result suggests that skew and synonymy are strongly related.
We also used log-likelihood to solve the TOEFL synonym questions. For each target-alternative pair, we calculated the log-likelihood for every distance in the range four to 750. The initial cut-off discarded the affinity caused by phrases containing both target and alternative words. The upper cut-off of 750 represents the average document size in the collection. The cumulative log-likelihood was then used as the score for each alternative, and we considered the best alternative the one with higher accumulated log-likelihood. With this approach, we are able to answer 86.25% of questions correctly, which is a substantial improvement over similar methods, which do not require training data.
5 Conclusion We presented a framework for the fast and effective computation of lexical affinity models. Instead of using arbitrary windows to compute word similarity measures, we model lexical affinity using the complete observed distance distribution along with independence and parametric models for this distribution. Our results shows that, with minimal effort to adapt the models, we achieve good results by applying this framework to simple natural language tasks, such as TOEFL synonym questions and GRE fill-in-the-blanks tests. This framework allows the use of terabyte-scale corpora by providing a fast algorithm to extract pairs of co-occurrence for the models, thus enabling the use of more precise estimators.
