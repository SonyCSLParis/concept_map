1 Introduction The IBM statistical machine translation (SMT) models have been extremely influential in computational linguistics in the past decade. The (arguably) most striking characteristic of the IBM-style SMT models is their total lack of linguistic knowledge. The IBM models demonstrated how much one can do with pure statistical techniques, which have inspired a whole new generation of NLP research and systems.
More recently, there have been many proposals to introduce syntactic knowledge into SMT models (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Lopez et al., 2002). A common theme among these approaches is the assumption that the syntactic structures of a pair of source-target sentences are isomorphic (or nearly isomorphic). This assumption seems too strong. Human translators often use non-literal translations, which result in differences in syntactic structures. According to a study in (Dorr et al., 2002), such translational divergences are quite common, involving 11-31% of the sentences.
We introduce a constraint that uses the dependency tree of the English sentence to maintain phrasal cohesion in the French sentence. In other words, if two phrases are disjoint in the English sentence, the alignment must not map them to overlapping intervals in the French sentence. For example, in Figure 1, the cohesion constraint will rule out the possibility of aligning to with a`. The phrases the reboot and the host to discover all the devices are disjoint, but the partial alignment in Figure 1 maps them to overlapping intervals. This constraint is weaker than isomorphism. However, we will show that it can produce a significant increase in alignment quality.
The reboot causes the host to discover all the devices det subj det subj aux pre det obj mod
à    la Suite réinitialisation  , l'  hôte repère tous les périphériques after to the reboot the host locate all the peripherals
1 2 3 4 5 6 7 8 9 10
1 2 3 4 5 6 7 8 9 10 11
Figure 1: A cohesion constraint violation
2 Cohesion Constraint Given an English sentence E = e1e2 . . . el and a French sentence F = f1f2 . . . fm, an alignment is a set of links between the words in E and F . An alignment can be represented as a binary relation A in [1, l] × [1,m]. A pair (i, j) is in A if ei and fj are a translation (or part of a translation) of each other. We call such pairs links. In Figure 2, the links in the alignment are represented by dashed lines.
The reboot causes the host to discover all the devices det subj det subj aux pre det obj comp
à    la Suite réinitialisation  , l'  hôte repère tous les périphériques 1 2 3 4 5 6 7 8 9 10
1 2 3 4 5 6 7 8 9 10 after to the reboot the host locate all the peripherals
Figure 2: An example pair of aligned sentence
The cohesion constraint (Fox, 2002) uses the dependency tree TE (Mel’cˇuk, 1987) of the English sentence to restrict possible link combinations. Let TE(ei) be the subtree of TE rooted at ei. The phrase span of ei, spanP (ei, TE , A), is the image of the English phrase headed by ei in F given a (partial) alignment A. More precisely, spanP (ei, TE , A) = [k1, k2], where k1 = min{j|(u, j) ∈ A, eu ∈ TE(ei)} k2 = max{j|(u, j) ∈ A, eu ∈ TE(ei)}
The head span is the image of ei itself. We define spanH(ei, TE , A) = [k1, k2], where k1 = min{j|(i, j) ∈ A} k2 = max{j|(i, j) ∈ A}
In Figure 2, the phrase span of the node discover is [6, 11] and the head span is [8, 8]; the phrase span of the node reboot is [3, 4] and the head span is [4, 4]. The word cause has a phrase span of [3,11] and its head span is the empty set ∅.
With these definitions of phrase and head spans, we define two notions of overlap, originally introduced in (Fox, 2002) as crossings. Given a head node eh and its modifier em, a head-modifier overlap occurs when: spanH(eh, TE , A) ∩ spanP (em, TE , A) 6= ∅ Given two nodes em1 and em2 which both modify the same head node, a modifier-modifier overlap occurs when: spanP (em1 , TE , A) ∩ spanP (em2 , TE , A) 6= ∅ Following (Fox, 2002), we say an alignment is cohesive with respect to TE if it does not introduce any headmodifier or modifier-modifier overlaps. For example, the alignment A in Figure 1 is not cohesive because there is an overlap between spanP (reboot, TE , A)=[4, 4] and spanP (discover, TE , A)=[2, 11].
If an alignmentA′ violates the cohesion constraint, any alignment A that is a superset of A′ will also violate the cohesion constraint. This is because any pair of nodes that have overlapping spans in A′ will still have overlapping spans in A.
Cohesion Checking Algorithm: We now present an algorithm that checks whether an individual link (ei, fj) causes a cohesion constraint violation when it is added to a partial alignment. Let ep0 , ep1 , ep2 , . . . be a sequence of nodes in TE such that ep0=ei and epk=parentOf (epk−1) (k = 1, 2, . . .)
1. For all k ≥ 0, update the spanP and the spanH of epk to include j.
2. For each epk (k > 0), check for a modifier-modifier overlap between the updated the phrase span of epk−1 and the the phrase span of each of the other children of epk .
3. For each epk (k > 0), check for a head-modifier overlap between the updated phrase span of epk−1 and the head span of epk .
4. If an overlap is found, return true (the constraint is violated). Otherwise, return false.
3 Evaluation To determine the utility of the cohesion constraint, we incorporated it into two alignment algorithms. The algorithms take as input an English-French sentence pair and the dependency tree of the English sentence. Both algorithms build an alignment by adding one link at a time. We implement two versions of each algorithm: one with the cohesion constraint and one without. We will describe the versions without cohesion constraint below. For the versions with cohesion constraint, it is understood that each new link must also pass the test described in Section 2.
The first algorithm is similar to Competitive Linking (Melamed, 1997). We use a sentence-aligned corpus to compute the φ2 correlation metric (Gale and Church, 1991) between all English-French word pairs. For a given sentence pair, we begin with an empty alignment. We then add links in the order of their φ2 scores so that each word participates in at most one link. We will refer to this as the φ2 method.
The second algorithm uses a best-first search (with fixed beam width and agenda size) to find an alignment that maximizes P (A|E,F ). A state in this search space is a partial alignment. A transition is defined as the addition of a single link to the current state. The algorithm computes P (A|E,F ) based on statistics obtained from a word-aligned corpus. We construct the initial corpus with a system that is similar to the φ2 method. The algorithm then re-aligns the corpus and trains again for three iterations. We will refer to this as the P (A|E,F ) method. The details of this algorithm are described in (Cherry and Lin, 2003).
We trained our alignment programs with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences. Both the training and testing sentences are from the Hansard corpus. We parsed the training and testing corpora with Minipar.1 We adopted the evaluation methodology in (Och and Ney, 2000), which defines three metrics: precision, recall and alignment error rate (AER).
Table 1 shows the results of our experiments. The first four rows correspond to the methods described above. As a reference point, we also provide the results reported in (Och and Ney, 2000). They implemented IBM Model 4 by bootstrapping from an HMM model. The rows F→E
1available at http://www.cs.ualberta.ca/˜ lindek/minipar.htm
Table 1: Evaluation Results Method Prec Rec AER
φ2 w/o cohesion 82.7 84.6 16.5 w/ cohesion 89.2 82.7 13.8
P (A|E,F ) w/o cohesion 87.3 85.3 13.6 w/ cohesion 95.7 86.4 8.7 F→E 80.5 91.2 15.6
Och&Ney E→F 80.0 90.8 16.0 Refined 85.9 92.3 11.7 and E→F are the results obtained by this model when treating French as the source and English as the target or vice versa. The row Refined shows results obtained by taking the intersection of E→F and F→E and then refining this intersection to increase recall.
From Table 1, we can see that the addition of the cohesion constraint leads to significant improvements in performance with both algorithms. The relative reduction in error rate is 16% with the φ2 method and 36% with the P (A|E,F ) method. The improvement comes primarily from increased precision. With the P (A|E,F ) method, this increase in precision does not come at the expense of recall.
4 Related Work
There has been a growing trend in the SMT community to attempt to leverage syntactic data in word alignment. Methods such as (Wu, 1997), (Alshawi et al., 2000) and (Lopez et al., 2002) employ a synchronous parsing procedure to constrain a statistical alignment. The work done in (Yamada and Knight, 2001) measures statistics on operations that transform a parse tree from one language into another.
The syntactic knowledge that is leveraged in these methods is tightly coupled with the alignment method itself. We have presented a modular constraint that can be plugged into different alignment algorithms. This has allowed us to test the contribution of the constraint directly.
(Fox, 2002) studied the extent to which the cohesion constraint holds in a parallel corpus and the reasons for the violations, but did not apply the constraint to an alignment algorithm.
5 Conclusion We have presented a syntax-based constraint for word alignment, known as the cohesion constraint. It requires disjoint English phrases to be mapped to non-overlapping intervals in the French sentence. Our experiments have shown that the use of this constraint can provide a relative reduction in alignment error rate of 36%.
